{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe5961b",
   "metadata": {},
   "source": [
    "# Context-Aware Chatbots\n",
    "\n",
    "In this example, we use the OpenAI SDK to communicate with an existing agent that has context awareness capabilities.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "* **Retrieval-Augmented Generation (RAG):** This is a hybrid approach in machine learning that combines information retrieval with generative models. It involves fetching relevant data from an external knowledge base (e.g., via vector similarity searches) and incorporating it into the prompt for a language model to generate informed responses. RAG improves factual accuracy by grounding outputs in retrieved context, rather than relying solely on the model's pre-trained knowledge.\n",
    "\n",
    "* **OpenAI Assistants:** These are configurable AI agents built on models like GPT, capable of maintaining conversation threads, executing tools, and processing user queries. They support features such as file uploads and retrieval, enabling context-aware interactions without assuming prior user knowledge.\n",
    "\n",
    "\n",
    "### Are OpenAI Assistants RAG Systems?\n",
    "\n",
    "Kind of. OpenAI Assistants incorporate RAG-like capabilities, particularly through their built-in retrieval tool. When enabled, this tool automatically indexes uploaded files and retrieves relevant chunks to augment the model's responses, functioning similarly to traditional RAG by enhancing generation with external data. However, they differ from manual RAG implementations (e.g., using custom vector databases like Pinecone) in that OpenAI handles the retrieval process internally, offering less granular control but greater ease of use. For instance, Assistants do not employ explicit vector similarity searches in the traditional sense but achieve comparable results via optimized indexing.\n",
    "\n",
    "This integration makes Assistants a practical form of RAG for applications like chatbots or knowledge-based querying, though custom RAG may be preferred for scenarios requiring fine-tuned retrieval parameters.\n",
    "\n",
    "> This demo assumes there is a valid OpenAI developer account with an existing assistant ready for use (although an assistant can be easily created programmatically). if you do not already have one, check the [API overview](https://platform.openai.com/docs/assistants/overview) and the [quickstart guide](https://platform.openai.com/docs/assistants/quickstart?example=without-streaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfb7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Go programming language was hatched in a secret Google lab by Ken Thompson, Rob Pike, and Robert Griesemer, possibly after they all got tired of wearing polka-dot lab coats and shouting “C++ is too complicated!” at each other. Legend has it they then bonded over suspiciously large quantities of coffee, resulting in the simple, speedy, gopher-friendly language we know and love today—Go. Some say the gopher mascot was actually invented to protect the last donut from being stolen in that caffeine-fueled frenzy. But that’s just speculation.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Creating client with API key.\n",
    "client = OpenAI(api_key=\"<YOUR API KEY>\")\n",
    "\n",
    "# Retrieving assistant.\n",
    "assistant = client.beta.assistants.retrieve(\"<YOUR ASSISTANT ID>\")\n",
    "\n",
    "# Creating thread. This starts a new conversation with the assistant.\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Messaging assistant.\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Who created the Go programming language?\"\n",
    ")\n",
    "\n",
    "# Initiating query.\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Polling response.\n",
    "while True:\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    if run.status == \"completed\":\n",
    "        break\n",
    "    elif run.status == \"failed\":\n",
    "        print(\"Run failed:\", run.last_error)\n",
    "\n",
    "        break\n",
    "    \n",
    "    time.sleep(2)  # Waiting 2 seconds before next check.\n",
    "\n",
    "# Response received, fetching from thread.\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "response = messages.data[0].content[0].text.value  # Latest message content.\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
