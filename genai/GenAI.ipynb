{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40022ef",
   "metadata": {},
   "source": [
    "# Generative RNNs\n",
    "\n",
    "Generative artificial intelligence (AI) refers to systems that create new data instances resembling a training dataset, such as text, images, or sequences. These models learn the underlying probability distribution of the data and sample from it to produce novel outputs. Recurrent neural networks (RNNs) are particularly effective for sequential data generation, such as text or time series, due to their ability to maintain memory of previous inputs through hidden states.\n",
    "\n",
    "## Applications and Limitations\n",
    "RNNs excel in tasks like text generation, where a model trained on a corpus (e.g., quotes) can produce new sentences by sampling from predicted distributions. Variants like Long Short-Term Memory (LSTM) networks address issues such as vanishing gradients in vanilla RNNs, improving performance on longer sequences.\n",
    "\n",
    "However, RNNs can suffer from mode collapse or repetitive outputs. Advanced generative frameworks, such as combining RNNs with variational autoencoders or adversarial training, enhance diversity and quality. Overall, RNN-based generative AI provides a foundational approach for sequence synthesis, bridging simple probabilistic modeling with complex creative applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5bca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated quote 1: he wornt wissey succetuple the and succeat is inthe is the the ave thenve the to who the man the.\n",
      "Generated quote 2: ere se't obed ssak succe thenlintho diifare the farang you dess.\n",
      "Generated quote 3: cise and whas on to do arentiress is the farl to succest wofot.\n",
      "Generated quote 4: ean the and whof y.\n",
      "Generated quote 5: are acire the it anland toantow dot to are success in tire fomlnis do ald waat you success.\n"
     ]
    }
   ],
   "source": [
    "from GenRNN import GenRNN\n",
    "from Quotes import sample_quotes\n",
    "\n",
    "# Initializing model.\n",
    "model = GenRNN(hidden_size=10, seq_length=100, learning_rate=0.1)\n",
    "\n",
    "# Training model.\n",
    "model.train(sample_quotes, epochs=10)\n",
    "\n",
    "# Generating quotes.\n",
    "for i in range(5):\n",
    "    quote = model.generate_quote(max_length=100, temperature=0.9, top_k=10)\n",
    "    print(f\"Generated quote {i+1}: {quote}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199aea1",
   "metadata": {},
   "source": [
    "# Takeaways from the Output\n",
    "\n",
    "What is interesting about the results above are not the outputs--especially because they are not English sentences. Instead, what is interesting is the model's ability to understand the statistical patterns in each words by trying to understand the relationships between characters and words in the sample data. By understanding these patterns, the model can generate new words that, to the best of its ability, are similar to the data on which it was trained.\n",
    "\n",
    "Even though the outputs are nothing but gibberish, some English words were produced, as well as words that do not exist, but resemble words in the English language. \n",
    "\n",
    "Real-life models are trained with hundreds of billions of samples, and are continually improved in order to be able to produce human-like outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
